{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fcd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dcde24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch. 12 Question Tests\n",
    "\n",
    "def q3test():\n",
    "    t1 = tf.range(10)\n",
    "    t2 = tf.constant(np.arange(10))\n",
    "    \n",
    "    print(t1)\n",
    "    print(t2)\n",
    "    \n",
    "    is_equal = (t1 == t2)\n",
    "    \n",
    "    print(is_equal)\n",
    "    \n",
    "    return t1, t2, is_equal\n",
    "\n",
    "def q12test(layernormalization):  # layernormalization - custom keras layer\n",
    "    # fake dataset\n",
    "    X = tf.constant([[1, 2, 3, 4],\n",
    "                     [5, 6, 7, 8]], dtype=tf.float32)\n",
    "    \n",
    "    print(X)\n",
    "\n",
    "    X_layernormed = tf.keras.layers.LayerNormalization()(X)\n",
    "    X_layernormed_custom = layernormalization(X)\n",
    "    \n",
    "    print(X_layernormed)\n",
    "    print(X_layernormed_custom)\n",
    "    \n",
    "    return X_layernormed, X_layernormed_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316806e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n",
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n",
      "tf.Tensor([ True  True  True  True  True  True  True  True  True  True], shape=(10,), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>,\n",
       " <tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>,\n",
       " <tf.Tensor: shape=(10,), dtype=bool, numpy=\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True])>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Cell\n",
    "\n",
    "q3test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d47d48",
   "metadata": {},
   "source": [
    "# Chapter 12 Exercises\n",
    "1.) TensorFlow is a deep learning framework and library that spans across multiple languages such as python, javascript, java, C++, and C#. Other popular Deep Learning frameworks are PyTorch, Theano, MXNET, Caffe, and more.\n",
    "\n",
    "2.) TensorFlow is not a drop-in replacement for NumPy. TensorFlow is a deep learning framework and library, while NumPy is a linear algebra library. The main differences between NumPy ndarray and TensorFlow Tensor are that Tensors are optimized to work with hardware and GPU acceleration and the TF Graph\n",
    "\n",
    "3.) Yes\n",
    "\n",
    "4.) Queue, Graph, RaggedTensor, String, TensorArray, Set\n",
    "\n",
    "5.) Use the regular function for most cases. If you need to have it dependent on the internals of the model/layers (like for regularization for example), subclass it.\n",
    "\n",
    "6.) In most cases, just use the regular function. If you need to have the metric dependent on model internals or need it to occur in a different way (such as not using the mean), subclass it.\n",
    "\n",
    "7.) Custom layer should be used when the layer could be repeated many times in the model (like a convolution block having conv then pool). Custom model should be made (subclassed) when you have some crazy architecture shit you wanna do\n",
    "\n",
    "8.) Some use cases that require writing a custom training loop: \n",
    "    - If you wanna do some crazy shit like use multiple optimizers\n",
    "    - Anything that can't be covered well by a regular training loop\n",
    "\n",
    "9.) They can contain arbitrary Python code, but it won't work as one might expect if one uses external library calls that can be replaced with a tf function (such as numpy). Instead, the python code will only run during tracing (which might be what you want, even if that is unlikely). Otherwise, it is a really good idea to just use TF functions. If you absolutely have to have the arbitrary python code that has external library calls, then you can wrap it in `tf.py_function()`\n",
    "\n",
    "10.) \n",
    "    - Refer to Question 9: External Library calls will only run during tracing, \n",
    "    - Use mainly tf functions, especially with loops\n",
    "    - Allow source code to be available to TF\n",
    "    - Vectorize\n",
    "    - If function creates a TF stateful object, it can only do it on the very first call. If you want to assign a new value to a variable, use `assign()` rather than `=`\n",
    "\n",
    "11.) When the model needs to be modified at runtime. Not all models should be dynamic, as they are slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f6df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3. 4.]\n",
      " [5. 6. 7. 8.]], shape=(2, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.3411044 -0.4470348  0.4470348  1.3411044]\n",
      " [-1.3411044 -0.4470348  0.4470348  1.3411044]], shape=(2, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.3416406  -0.44721353  0.44721353  1.3416406 ]\n",
      " [-1.3416406  -0.44721353  0.44721353  1.3416406 ]], shape=(2, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[-1.3411044, -0.4470348,  0.4470348,  1.3411044],\n",
       "        [-1.3411044, -0.4470348,  0.4470348,  1.3411044]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[-1.3416406 , -0.44721353,  0.44721353,  1.3416406 ],\n",
       "        [-1.3416406 , -0.44721353,  0.44721353,  1.3416406 ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q12\n",
    "\n",
    "\"\"\"\n",
    "This is meant to be a clone of tf.keras.layers.LayerNormalization. How is this different from BatchNormalization?\n",
    "While BatchNormalization does it across an entire mini batch, LayerNormalization does it across all the dimensions, or neurons, in that layer\n",
    "\n",
    "Note: this will be tested in one of the following exercises\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = K.epsilon()\n",
    "    \n",
    "    def build(self, input_shape):  # input_shape is of the batch\n",
    "        self.alpha = self.add_weight(shape=input_shape[-1:],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=\"ones\",\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(shape=input_shape[-1:],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=\"zeros\",\n",
    "                                     trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        (mean, variance) = tf.nn.moments(inputs, axes=-1, keepdims=True)\n",
    "        \n",
    "        std = tf.math.sqrt(variance)\n",
    "        \n",
    "        standardized_inputs = (inputs - mean) / (std + self.epsilon)\n",
    "        \n",
    "        return (self.alpha * standardized_inputs) + self.beta\n",
    "    \n",
    "    \n",
    "# Testing time\n",
    "q12test(LayerNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9310195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResult: Close enough!\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Result: Close enough!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c31e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for Q13\n",
    "\n",
    "def tensorfy(X, y, dtype=tf.float32):\n",
    "    X_tensor = tf.convert_to_tensor(X, dtype=dtype)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=dtype)\n",
    "    \n",
    "    return (X, y)\n",
    "\n",
    "def square_img_classification_model(IMG_SIZE, NUM_CLASSES,\n",
    "                                    FC_UNITS=100, LAST_CONV_ACTIVATION=Activation(\"swish\")):\n",
    "    conv1_units = 32\n",
    "    conv2_units = conv1_units * 2\n",
    "    conv2_flattened_dim = conv2_units * ((IMG_SIZE - 2)**2)\n",
    "    \n",
    "    # LAYERS\n",
    "    input_layer = Input(shape=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    flattened_input = Flatten()(input_layer)\n",
    "    flattened_input = BatchNormalization()(flattened_input)\n",
    "\n",
    "    encode = Dense(IMG_SIZE*IMG_SIZE, use_bias=False)(flattened_input)\n",
    "    encode = Reshape(target_shape=(IMG_SIZE, IMG_SIZE, 1))(encode)\n",
    "\n",
    "    conv1 = Conv2D(conv1_units, kernel_size=(2, 2), padding=\"same\", strides=(1, 1))(encode)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "\n",
    "    conv2 = Conv2D(conv2_units, kernel_size=(3, 3), padding=\"valid\", strides=(1, 1), use_bias=False)(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = LAST_CONV_ACTIVATION(conv2)\n",
    "    \n",
    "    # flatten conv2\n",
    "    conv2_flattened = Flatten()(conv2)\n",
    "\n",
    "    # SE portion\n",
    "    SE2 = GlobalAveragePooling2D()(conv2)\n",
    "    SE2 = Dense(FC_UNITS, activation=\"relu\", kernel_initializer=\"he_normal\")(SE2)\n",
    "    SE2 = Dense(conv2_flattened_dim, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\")(SE2)  # logic gate\n",
    "\n",
    "    # combine\n",
    "    fc1_in = Multiply()([SE2, conv2_flattened])\n",
    "\n",
    "    fc1 = Dense(FC_UNITS, kernel_initializer=\"he_uniform\")(fc1_in)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    \n",
    "    fc2 = Dense(units=max(1, int(0.75 * FC_UNITS)), use_bias=False)(fc1)\n",
    "    fc2 = BatchNormalization()(fc2)\n",
    "    fc2 = Activation(\"elu\")(fc2)\n",
    "\n",
    "    output_layer = Dense(NUM_CLASSES, activation=\"linear\")(fc2)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "\n",
    "def print_status_bar(iteration, total_iterations_per_epoch, metrics=[]):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in metrics])\n",
    "    end = \"\" if iteration < total_iterations_per_epoch else \"\\n\"\n",
    "    \n",
    "    print(\"\\r{}/{} - \".format(iteration, total_iterations_per_epoch) + metrics, end=end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13e89aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation architectures\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "def square_img_classification_model_experimental(IMG_SIZE, NUM_CLASSES,\n",
    "                                                 FC_UNITS=100, LAST_CONV_ACTIVATION=Activation(\"swish\")):\n",
    "    conv1_units = 32\n",
    "    conv2_units = conv1_units * 2\n",
    "    conv2_flattened_dim = conv2_units * ((IMG_SIZE - 2)**2)\n",
    "    \n",
    "    FC_UNITS_2 = max(1, int(0.75 * FC_UNITS))\n",
    "    \n",
    "    # LAYERS\n",
    "    input_layer = Input(shape=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    flattened_input = Flatten()(input_layer)\n",
    "    flattened_input = BatchNormalization()(flattened_input)\n",
    "\n",
    "    encode = Dense(IMG_SIZE*IMG_SIZE, use_bias=False, activation=\"linear\")(flattened_input)\n",
    "    encode_reshaped = Reshape(target_shape=(IMG_SIZE, IMG_SIZE, 1))(encode)\n",
    "\n",
    "    conv1 = Conv2D(conv1_units, kernel_size=(2, 2), padding=\"same\", strides=(1, 1))(encode_reshaped)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "\n",
    "    conv2 = Conv2D(conv2_units, kernel_size=(3, 3), padding=\"valid\", strides=(1, 1), use_bias=False)(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = LAST_CONV_ACTIVATION(conv2)\n",
    "    \n",
    "    # flatten conv2\n",
    "    conv2_flattened = Flatten()(conv2)\n",
    "\n",
    "    # SE portion\n",
    "    SE2 = GlobalAveragePooling2D()(conv2)\n",
    "    SE2 = Dense(FC_UNITS, activation=\"relu\", kernel_initializer=\"he_normal\")(SE2)\n",
    "    SE2 = Dense(conv2_flattened_dim, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\")(SE2)  # logic gate\n",
    "\n",
    "    # combine\n",
    "    fc1_in = Multiply()([SE2, conv2_flattened])\n",
    "\n",
    "    fc1 = Dense(FC_UNITS, kernel_initializer=\"he_uniform\", use_bias=False)(fc1_in)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    \n",
    "    fc2 = Dense(FC_UNITS_2, use_bias=False)(fc1)\n",
    "    fc2 = BatchNormalization()(fc2)\n",
    "    fc2 = Activation(\"swish\")(fc2)\n",
    "    \n",
    "    fc3 = Dense(FC_UNITS_2, kernel_regularizer=l1(0.01))(layers.concatenate([fc2, conv2_flattened, encode]))\n",
    "    fc3 = Activation(\"gelu\")(fc3)\n",
    "\n",
    "    output_layer = Dense(NUM_CLASSES, activation=\"linear\")(layers.concatenate([fc2, fc3]))\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_layer], outputs=[output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "069f11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "781/781 - average cost: 0.3868 - average val cost: 0.4569 - mean_absolute_error: 2.7571\n",
      "\n",
      "Epoch 2/10\n",
      "781/781 - average cost: 0.2596 - average val cost: 0.4660 - mean_absolute_error: 2.9585\n",
      "\n",
      "Epoch 3/10\n",
      "781/781 - average cost: 0.4584 - average val cost: 0.5510 - mean_absolute_error: 2.8548\n",
      "\n",
      "Epoch 4/10\n",
      "781/781 - average cost: 0.4001 - average val cost: 0.3835 - mean_absolute_error: 3.0699\n",
      "\n",
      "Epoch 5/10\n",
      "781/781 - average cost: 0.4145 - average val cost: 0.2005 - mean_absolute_error: 3.2152\n",
      "\n",
      "Epoch 6/10\n",
      "781/781 - average cost: 0.3652 - average val cost: 0.3629 - mean_absolute_error: 3.3460\n",
      "\n",
      "Epoch 7/10\n",
      "781/781 - average cost: 0.0995 - average val cost: 0.4516 - mean_absolute_error: 3.3317\n",
      "\n",
      "Epoch 8/10\n",
      "781/781 - average cost: 0.3108 - average val cost: 0.2723 - mean_absolute_error: 3.4423\n",
      "\n",
      "Epoch 9/10\n",
      "781/781 - average cost: 0.1028 - average val cost: 0.3733 - mean_absolute_error: 3.5038\n",
      "\n",
      "Epoch 10/10\n",
      "781/781 - average cost: 0.1188 - average val cost: 0.2389 - mean_absolute_error: 3.4172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q13 (a)\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import Dataset\n",
    "from keras.layers import Input, Flatten, Reshape, Multiply\n",
    "from keras.layers import Dense, Conv2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.layers import Activation, PReLU\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "assert X_train.shape == (60000, IMG_SIZE, IMG_SIZE)\n",
    "assert X_test.shape == (10000, IMG_SIZE, IMG_SIZE)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)\n",
    "\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "num_classes = len(labels)\n",
    "assert num_classes == 10\n",
    "\n",
    "# divide by max value\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# create a validation set\n",
    "val_ratio = len(y_test) / len(y_train)\n",
    "(X_train, X_val, y_train, y_val) = train_test_split(X_train, y_train, test_size=val_ratio)\n",
    "\n",
    "# convert to tensors\n",
    "(X_train, y_train) = tensorfy(X_train, y_train)\n",
    "(X_val, y_val) = tensorfy(X_val, y_val)\n",
    "(X_test, y_test) = tensorfy(X_test, y_test)\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = tf.one_hot(y_train, depth=num_classes)\n",
    "y_val = tf.one_hot(y_val, depth=num_classes)\n",
    "y_test = tf.one_hot(y_test, depth=num_classes)\n",
    "\n",
    "def make_and_prepare_dataset(X, y, batch_size, prefetch=True, buffer_size=2000):  # we want the buffer size to be bigger than batch size\n",
    "    dataset_X = Dataset.from_tensor_slices(X)\n",
    "    dataset_y = Dataset.from_tensor_slices(y)\n",
    "    \n",
    "    # combine\n",
    "    dataset = Dataset.zip((dataset_X, dataset_y))\n",
    "    \n",
    "    # prepare\n",
    "    dataset = (dataset\n",
    "                .shuffle(buffer_size=buffer_size)\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_model(model, X_y: tuple, Xval_yval: tuple, \n",
    "                loss, optimizer, metrics=[],\n",
    "                epochs=1, batch_size=32, starting_epoch=1):\n",
    "    (X, y) = X_y\n",
    "    (X_val, y_val) = Xval_yval\n",
    "    \n",
    "    '''\n",
    "    An epoch is when it sees ALL the examples in training set regardless of batch size\n",
    "    So with Batch Gradient Descent, 1 epoch = 1 iteration\n",
    "    ''' \n",
    "    ending_epoch = starting_epoch + epochs - 1\n",
    "    steps_per_epoch = len(X) // batch_size  \n",
    "    #total_steps = epochs * steps_per_epoch\n",
    "\n",
    "    train_set = make_and_prepare_dataset(X, y, batch_size)\n",
    "    val_set = make_and_prepare_dataset(X_val, y_val, batch_size)\n",
    "    val_set_length = len(val_set)\n",
    "    \n",
    "    average_train_cost = keras.metrics.Mean(\"average cost\")\n",
    "    average_val_cost = keras.metrics.Mean(\"average val cost\")\n",
    "    \n",
    "    all_metrics = [average_train_cost, average_val_cost]\n",
    "    for metric in metrics:\n",
    "        all_metrics.append(metric)\n",
    "    \n",
    "    def forward_pass(X, y):\n",
    "        yhat = model(X, training=True)\n",
    "        costs = tf.reduce_mean(loss(y, yhat))\n",
    "        \n",
    "        return (yhat, costs)\n",
    "    \n",
    "    for epoch in range(starting_epoch, ending_epoch + 1):\n",
    "        print(f\"Epoch {epoch}/{ending_epoch}\")\n",
    "        step = 0\n",
    "        \n",
    "        for (X_minibatch, y_minibatch) in train_set.take(steps_per_epoch):\n",
    "            random_val_index = int(np.random.rand() * val_set_length)\n",
    "            (X_val_minibatch, y_val_minibatch) = iter(val_set.skip(random_val_index)).get_next()\n",
    "            with tf.GradientTape() as tape:\n",
    "                # TAPE THE FORWARD PASS (RECORD)\n",
    "                yhat, costs = forward_pass(X_minibatch, y_minibatch)\n",
    "            \n",
    "            # NOW FOR THE BACKWARD PASS\n",
    "            gradients = tape.gradient(costs, model.trainable_variables)           # backpropagation\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # LEARNING STEP!!!\n",
    "            \n",
    "            # Validation Forward Pass\n",
    "            yhat_val, costs_val = forward_pass(X_val_minibatch, y_val_minibatch)\n",
    "            \n",
    "            # Average Costs for this learning step (update states)\n",
    "            average_train_cost(costs)   # before learning step\n",
    "            average_val_cost(costs_val) # after learning step\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            for metric in metrics:\n",
    "                metric(y_minibatch, yhat)\n",
    "            \n",
    "            step += 1\n",
    "            print_status_bar(step, steps_per_epoch, all_metrics)\n",
    "            \n",
    "            for metric in all_metrics:\n",
    "                metric.reset_states()\n",
    "        print()\n",
    "\n",
    "# Create Model\n",
    "model = square_img_classification_model(IMG_SIZE=28, NUM_CLASSES=num_classes)\n",
    "\n",
    "# Instead of compiling and fitting, we are training manually\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "train_model(model, (X_train, y_train), (X_val, y_val), \n",
    "            loss, optimizer, metrics,\n",
    "            epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1864a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "781/781 - average cost: 0.2446 - average val cost: 0.3510 - mean_absolute_error: 3.0260\n",
      "\n",
      "Epoch 2/10\n",
      "781/781 - average cost: 0.3377 - average val cost: 0.3659 - mean_absolute_error: 3.4326\n",
      "\n",
      "Epoch 3/10\n",
      "781/781 - average cost: 0.3190 - average val cost: 0.5703 - mean_absolute_error: 3.7836\n",
      "\n",
      "Epoch 4/10\n",
      "781/781 - average cost: 0.3369 - average val cost: 0.4635 - mean_absolute_error: 3.5231\n",
      "\n",
      "Epoch 5/10\n",
      "781/781 - average cost: 0.1789 - average val cost: 0.4834 - mean_absolute_error: 3.4863\n",
      "\n",
      "Epoch 6/10\n",
      "781/781 - average cost: 0.2113 - average val cost: 0.2553 - mean_absolute_error: 3.5692\n",
      "\n",
      "Epoch 7/10\n",
      "641/781 - average cost: 0.3618 - average val cost: 0.2919 - mean_absolute_error: 3.9494"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 80>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m optimizers \u001b[38;5;241m=\u001b[39m [Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m), keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.008\u001b[39m), keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mNadam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)]\n\u001b[0;32m     78\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanAbsoluteError()]\n\u001b[1;32m---> 80\u001b[0m \u001b[43mtrain_crazy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrazy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36mtrain_crazy_model\u001b[1;34m(crazy_model, X_y, Xval_yval, loss, optimizers, metrics, epochs, batch_size, starting_epoch)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[0;32m     53\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizers[\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m*\u001b[39m num_optimizers)]\n\u001b[1;32m---> 54\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrazy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Validation Forward Pass\u001b[39;00m\n\u001b[0;32m     57\u001b[0m yhat_val, costs_val \u001b[38;5;241m=\u001b[39m forward_pass(X_val_minibatch, y_val_minibatch)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:678\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    675\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_gradients(grads_and_vars)\n\u001b[0;32m    676\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_gradients(grads_and_vars)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:723\u001b[0m, in \u001b[0;36mOptimizerV2._distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, apply_state, name)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mcolocate_vars_with(var):\n\u001b[0;32m    720\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m name_scope_only_in_function_or_graph(\n\u001b[0;32m    721\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eagerly_outside_functions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    722\u001b[0m       var\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m--> 723\u001b[0m     update_op \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39min_cross_replica_context():\n\u001b[0;32m    726\u001b[0m       \u001b[38;5;66;03m# In cross-replica context, extended.update returns a list of\u001b[39;00m\n\u001b[0;32m    727\u001b[0m       \u001b[38;5;66;03m# update ops from all replicas (group=False).\u001b[39;00m\n\u001b[0;32m    728\u001b[0m       update_ops\u001b[38;5;241m.\u001b[39mextend(update_op)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2630\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2627\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   2628\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2629\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 2630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2632\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   2633\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3703\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   3701\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   3702\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 3703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3709\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   3705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   3706\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   3707\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   3708\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 3709\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[0;32m   3711\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\AI\\ML_Jungle\\env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:706\u001b[0m, in \u001b[0;36mOptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_apply_args:\n\u001b[0;32m    705\u001b[0m   apply_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m apply_state\n\u001b[1;32m--> 706\u001b[0m update_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_apply_dense(grad, var, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mapply_kwargs)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var\u001b[38;5;241m.\u001b[39mconstraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    708\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies([update_op]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Q13 (b)\n",
    "# Using a different optimizer for the lower layers\n",
    "\n",
    "def train_crazy_model(crazy_model, X_y: tuple, Xval_yval: tuple,\n",
    "                      loss, optimizers=[], metrics=[],\n",
    "                      epochs=1, batch_size=32, starting_epoch=1):\n",
    "    (X, y) = X_y\n",
    "    (X_val, y_val) = Xval_yval\n",
    "    \n",
    "    num_optimizers = len(optimizers)\n",
    "    \n",
    "    '''\n",
    "    An epoch is when it sees ALL the examples in training set regardless of batch size\n",
    "    So with Batch Gradient Descent, 1 epoch = 1 iteration\n",
    "    ''' \n",
    "    ending_epoch = starting_epoch + epochs - 1\n",
    "    steps_per_epoch = len(X) // batch_size  \n",
    "    #total_steps = epochs * steps_per_epoch\n",
    "\n",
    "    train_set = make_and_prepare_dataset(X, y, batch_size)\n",
    "    val_set = make_and_prepare_dataset(X_val, y_val, batch_size)\n",
    "    val_set_length = len(val_set)\n",
    "    \n",
    "    average_train_cost = keras.metrics.Mean(\"average cost\")\n",
    "    average_val_cost = keras.metrics.Mean(\"average val cost\")\n",
    "    \n",
    "    all_metrics = [average_train_cost, average_val_cost]\n",
    "    for metric in metrics:\n",
    "        all_metrics.append(metric)\n",
    "        \n",
    "    def forward_pass(X, y):\n",
    "        yhat = crazy_model(X)\n",
    "        costs = tf.reduce_mean(loss(y, yhat))\n",
    "        \n",
    "        return yhat, costs\n",
    "    \n",
    "    for epoch in range(starting_epoch, ending_epoch+1):\n",
    "        print(f\"Epoch {epoch}/{ending_epoch}\")\n",
    "        step = 0\n",
    "        \n",
    "        for (X_minibatch, y_minibatch) in train_set.take(steps_per_epoch):\n",
    "            random_val_index = int(np.random.rand() * val_set_length)\n",
    "            (X_val_minibatch, y_val_minibatch) = iter(val_set.skip(random_val_index)).get_next()\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward Pass\n",
    "                yhat, costs = forward_pass(X_minibatch, y_minibatch)\n",
    "            \n",
    "            # Backpropagation\n",
    "            gradients = tape.gradient(costs, crazy_model.trainable_variables)\n",
    "            \n",
    "            # Learn\n",
    "            optimizer = optimizers[int(np.random.rand() * num_optimizers)]\n",
    "            optimizer.apply_gradients(zip(gradients, crazy_model.trainable_variables))\n",
    "            \n",
    "            # Validation Forward Pass\n",
    "            yhat_val, costs_val = forward_pass(X_val_minibatch, y_val_minibatch)\n",
    "            \n",
    "            # Average Costs for this learning step (update states)\n",
    "            average_train_cost(costs)   # before learning step\n",
    "            average_val_cost(costs_val) # after learning step\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            for metric in metrics:\n",
    "                metric(y_minibatch, yhat)\n",
    "            \n",
    "            step += 1\n",
    "            print_status_bar(step, steps_per_epoch, all_metrics)\n",
    "            \n",
    "            for metric in all_metrics:\n",
    "                metric.reset_states()\n",
    "        print()\n",
    "\n",
    "crazy_model = square_img_classification_model(IMG_SIZE=28, NUM_CLASSES=num_classes)\n",
    "\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "optimizers = [Adam(learning_rate=0.005), keras.optimizers.SGD(learning_rate=0.008), keras.optimizers.Nadam(learning_rate=0.001)]\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "train_crazy_model(crazy_model, (X_train, y_train), (X_val, y_val), \n",
    "                  loss, optimizers, metrics,\n",
    "                  epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "edb89b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 82s 104ms/step - loss: 0.0073 - mean_absolute_error: 5.8910 - val_loss: 0.5769 - val_mean_absolute_error: 6.0793\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 82s 105ms/step - loss: 0.0037 - mean_absolute_error: 6.1292 - val_loss: 0.6150 - val_mean_absolute_error: 6.2801\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.0028 - mean_absolute_error: 6.3264 - val_loss: 0.6351 - val_mean_absolute_error: 6.3959\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.0032 - mean_absolute_error: 6.4833 - val_loss: 0.6707 - val_mean_absolute_error: 6.6781\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.0026 - mean_absolute_error: 6.6330 - val_loss: 0.6774 - val_mean_absolute_error: 6.7466\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.0030 - mean_absolute_error: 6.7336 - val_loss: 0.6920 - val_mean_absolute_error: 6.7514\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 81s 103ms/step - loss: 0.0032 - mean_absolute_error: 6.8142 - val_loss: 0.7025 - val_mean_absolute_error: 6.9325\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.0024 - mean_absolute_error: 6.9219 - val_loss: 0.7225 - val_mean_absolute_error: 7.0322\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 0.0025 - mean_absolute_error: 7.0640 - val_loss: 0.7331 - val_mean_absolute_error: 7.2594\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 81s 104ms/step - loss: 0.0022 - mean_absolute_error: 7.1944 - val_loss: 0.7445 - val_mean_absolute_error: 7.2639\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q13 (a) in Keras\n",
    "model_keras = square_img_classification_model_experimental(IMG_SIZE=28, NUM_CLASSES=num_classes)\n",
    "\n",
    "model.compile(loss=loss, optimizer=Adam(learning_rate=0.001), metrics=metrics)\n",
    "\n",
    "model_history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "# Plot history\n",
    "pd.DataFrame(model_history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6c59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
